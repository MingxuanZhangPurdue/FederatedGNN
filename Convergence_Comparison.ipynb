{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import copy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from GFL import MLP, LR, Node, Central_Server\n",
    "from utils import calculate_Atilde, mean_agg, cSBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = 10\n",
    "d = 5\n",
    "mu = 1\n",
    "l = 2\n",
    "csbm = cSBM(N, p, d, mu, l)\n",
    "A_tilde = calculate_Atilde(csbm.A, 100, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GFL_cSBM(csbm, A_tilde, hidden_dim, num_train, I, n_k, \n",
    "               num_communication=20, aggregation=mean_agg,\n",
    "               learning_rate=0.1, opt=\"Adam\", num_epochs=10,\n",
    "               gradient=True, m=10, gradient_clipping=None,\n",
    "               nn_type=\"MLP\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    csbm: An cSBM object (contextual stochastic block model)\n",
    "    \n",
    "    A_tilde: pageRank matrix\n",
    "    \n",
    "    n_k: number of feature vectors each node has.\n",
    "    \n",
    "    I: number of local updates for each node, so batch size = n_k/I for each node k.\n",
    "    \n",
    "    num_train: Number of nodes used in training.\n",
    "    \n",
    "    aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "    \n",
    "    num_communication: Number of communicatons. Default: 20\n",
    "    \n",
    "    learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "    opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "    \n",
    "    gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "    \n",
    "    m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "       Default: 10\n",
    "       \n",
    "    gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                       if a number (int or float) is given, \n",
    "                       then the maximum norm is determined by this number. Default: None.\n",
    "                       \n",
    "    nn_type: The type of neural network. either \"MLP\" or \"LR\" (i.e. MLP or Logistic Regression). Default:\"MLP\".\n",
    "    \"\"\"\n",
    "    \n",
    "    N = A_tilde.shape[0]\n",
    "    \n",
    "    input_dim = csbm.p\n",
    "    \n",
    "    output_dim = 2\n",
    "    \n",
    "    node_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            model_i = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            model_i = LR(input_dim, output_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Type of neural network must be either LR or MLP!\")\n",
    "            \n",
    "        \n",
    "        for j in range(n_k):\n",
    "            \n",
    "            x_j = np.sqrt(csbm.mu/N)*csbm.v[i]*csbm.u + np.random.normal(loc=0, scale=1, size=csbm.p)/np.sqrt(csbm.p)\n",
    "            \n",
    "            X.append(x_j)\n",
    "            \n",
    "        X = torch.tensor(np.array(X))\n",
    "        \n",
    "        if csbm.v[i] == -1:\n",
    "            \n",
    "            y = np.zeros(n_k)\n",
    "            \n",
    "        elif csbm.v[i] == 1:\n",
    "            \n",
    "            y = np.ones(n_k)\n",
    "\n",
    "        y = torch.tensor(y).type(torch.LongTensor)\n",
    "        \n",
    "        node_i = Node(local_model=model_i, node_idx=i, X=X, y=y)\n",
    "        \n",
    "        node_list.append(node_i)\n",
    "        \n",
    "    server = Central_Server(node_list, A_tilde)\n",
    "    \n",
    "    server.init_central_parameters(input_dim, hidden_dim, output_dim, nn_type)\n",
    "    \n",
    "    class1_train = np.random.choice(a=csbm.class1_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    class2_train = np.random.choice(a=csbm.class2_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    train_indices = np.concatenate((class1_train, class2_train), axis=0)\n",
    "    \n",
    "    test_indices = list(set(np.arange(N)) - set(train_indices))\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for ith in range(num_communication):\n",
    "        \n",
    "        average_train_loss = server.communication(train_indices, test_indices, \n",
    "                                                  I, aggregation, opt, learning_rate, num_epochs,\n",
    "                                                  gradient, m, gradient_clipping)\n",
    "        train_loss.append(average_train_loss)\n",
    "        \n",
    "        if (num_communication <= 30):\n",
    "            print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "            \n",
    "        elif (num_communication > 30 and num_communication <= 100):\n",
    "            if (ith % 5 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "                \n",
    "        elif (num_communication >= 10000):\n",
    "            if (ith % 100 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "                \n",
    "        else:\n",
    "            if (ith % 10 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "                \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1 Average train loss: 0.6935615539550781\n",
      "Communication: 101 Average train loss: 0.6922706365585327\n",
      "Communication: 201 Average train loss: 0.6947137713432312\n",
      "Communication: 301 Average train loss: 0.6900880336761475\n",
      "Communication: 401 Average train loss: 0.6885721683502197\n",
      "Communication: 501 Average train loss: 0.6888320446014404\n",
      "Communication: 601 Average train loss: 0.6833720207214355\n",
      "Communication: 701 Average train loss: 0.68278968334198\n",
      "Communication: 801 Average train loss: 0.6803514361381531\n",
      "Communication: 901 Average train loss: 0.6806635856628418\n",
      "Communication: 1001 Average train loss: 0.6792182326316833\n",
      "Communication: 1101 Average train loss: 0.6773951649665833\n",
      "Communication: 1201 Average train loss: 0.6787551641464233\n",
      "Communication: 1301 Average train loss: 0.6777995228767395\n",
      "Communication: 1401 Average train loss: 0.6763874888420105\n",
      "Communication: 1501 Average train loss: 0.6725531220436096\n",
      "Communication: 1601 Average train loss: 0.6692662835121155\n",
      "Communication: 1701 Average train loss: 0.6656981706619263\n",
      "Communication: 1801 Average train loss: 0.6715002655982971\n",
      "Communication: 1901 Average train loss: 0.6683152914047241\n",
      "Communication: 2001 Average train loss: 0.6658689975738525\n",
      "Communication: 2101 Average train loss: 0.666762113571167\n",
      "Communication: 2201 Average train loss: 0.6596466302871704\n",
      "Communication: 2301 Average train loss: 0.6663655638694763\n",
      "Communication: 2401 Average train loss: 0.6622487902641296\n",
      "Communication: 2501 Average train loss: 0.6580712795257568\n",
      "Communication: 2601 Average train loss: 0.6496415138244629\n",
      "Communication: 2701 Average train loss: 0.6634440422058105\n",
      "Communication: 2801 Average train loss: 0.6634740233421326\n",
      "Communication: 2901 Average train loss: 0.6547439694404602\n",
      "Communication: 3001 Average train loss: 0.6523924469947815\n",
      "Communication: 3101 Average train loss: 0.644612729549408\n",
      "Communication: 3201 Average train loss: 0.6485084295272827\n",
      "Communication: 3301 Average train loss: 0.6519803404808044\n",
      "Communication: 3401 Average train loss: 0.6516497135162354\n",
      "Communication: 3501 Average train loss: 0.6551440954208374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-be9d2b4a0b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m            \u001b[0mI\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_communication\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m            \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m            gradient=True, m=20, gradient_clipping=None, nn_type=\"LR\")\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-09f94da24d03>\u001b[0m in \u001b[0;36mtrain_GFL_cSBM\u001b[1;34m(csbm, A_tilde, hidden_dim, num_train, I, n_k, num_communication, aggregation, learning_rate, opt, num_epochs, gradient, m, gradient_clipping, nn_type)\u001b[0m\n\u001b[0;32m     98\u001b[0m         average_train_loss = server.communication(train_indices, test_indices, \n\u001b[0;32m     99\u001b[0m                                                   \u001b[0mI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                                                   gradient, m, gradient_clipping)\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_train_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FederatedGNN\\GFL.py\u001b[0m in \u001b[0;36mcommunication\u001b[1;34m(self, train_indices, test_indices, I, aggregation, opt, learning_rate, num_epochs, gradient, m, gradient_clipping)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m# H: [N, num_class]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_hs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;31m# C: [N, num_class]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FederatedGNN\\GFL.py\u001b[0m in \u001b[0;36mcollect_hs\u001b[1;34m(self, gradient)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 \u001b[0mh_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_h\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m                 \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[0mdH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdh_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FederatedGNN\\GFL.py\u001b[0m in \u001b[0;36mupload_h\u001b[1;34m(self, gradient)\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                         \u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                         \u001b[0mdh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tl = train_GFL_cSBM(csbm=csbm, A_tilde=A_tilde, hidden_dim=100, n_k=40,\n",
    "           I=1, num_communication=20000, aggregation=mean_agg, num_train=10, \n",
    "           num_epochs=1,\n",
    "           gradient=True, m=20, gradient_clipping=None, nn_type=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
