{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import copy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils import calculate_Atilde, cSBM, mean_agg, MLP, LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \n",
    "    def __init__(self, local_model, node_idx, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        local model: The local MLP model for each node\n",
    "        node_idx: The unique index of a node\n",
    "        X: [n_k, p], feature matrix, float tensor\n",
    "        y: [n_k], true labels, long tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = local_model\n",
    "        self.idx = node_idx\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_k = X.shape[0]\n",
    "        self.dataloader = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        \n",
    "    def upload_local_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Upload local model parameters to central server.\n",
    "        Usually used for aggregation step in each communication.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    \n",
    "    def receieve_central_parameters(self, central_parameters):\n",
    "        \n",
    "        \"\"\"\n",
    "        central_parameters: A state dictonary for central server parameters.\n",
    "        \n",
    "        Receive the broadcasted central parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pname, param in self.model.named_parameters():\n",
    "                param.copy_(central_parameters[pname])\n",
    "                \n",
    "                \n",
    "    def upload_h(self, gradient=True):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        This function uploads an random hidden vector from a node to the central server.\n",
    "        It also calculate and upload a dictonary of gradients  (dh/dw, 3D tensors) for each parameter w.r.t the local model\n",
    "        \"\"\" \n",
    "        \n",
    "        x = self.X[np.random.choice(a=self.n_k),:]\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            # Clear the possible accumulated gradient of the parameters of local model\n",
    "            self.model.zero_grad()\n",
    "        \n",
    "            h = self.model(x).view(1, -1)\n",
    "            \n",
    "            num_class = h.shape[-1]\n",
    "\n",
    "            dh = {}\n",
    "\n",
    "            for i in range(num_class):\n",
    "\n",
    "                h[0, i].backward(retain_graph=True)\n",
    "\n",
    "                for pname, param in self.model.named_parameters():\n",
    "\n",
    "                    if pname in dh:\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "                    else:\n",
    "                        dh[pname] = []\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "\n",
    "                    if (i == num_class-1):\n",
    "                        d1, d2 = dh[pname][0].shape\n",
    "                        dh[pname] = torch.cat(dh[pname], dim=0).view(num_class, d1, d2)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            return h, dh\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                h = self.model(x).view(1, -1)\n",
    "                \n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def upload_data(self, m=1):\n",
    "        \n",
    "        # Upload the m number of local data for evaluation purpose.\n",
    "        \n",
    "        if (m > self.n_k):\n",
    "            raise ValueError(\"m is bigger than n_k!\")\n",
    "            \n",
    "        ids = np.random.choice(a=self.n_k, size=m, replace=False)\n",
    "        \n",
    "        X = self.X[ids,:].view(m, 1, -1)\n",
    "        \n",
    "        y = self.y[ids].view(m, 1)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def local_update(self, A_tilde_k, C_k, dH, I, \n",
    "                     opt=\"Adam\",\n",
    "                     learning_rate=0.01, num_epochs=10, \n",
    "                     gradient=True, gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        The local update process for a node k.\n",
    "        \n",
    "        A_tilde_k: The kth row of PageRank matrix A_tilde.\n",
    "        \n",
    "        C_k: [1, num_class] The aggregated neighborhood information for node k.\n",
    "        \n",
    "        dH: A list of gradient dictonaries, where the kth dictonary contains the gradients of each parameter for node k.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        opt: Optimizer used for local updates: SGD or Adam. Default: \"Adam\"\n",
    "        \n",
    "        learning rate: learning rate for SGD. Default: 0.1\n",
    "        \n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "        \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "        if a number (int or float) is given, then the maximum norm is determined by this number. Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (self.dataloader == None):\n",
    "            batch_size = int(np.floor(self.n_k/I))\n",
    "            dataset = torch.utils.data.TensorDataset(self.X, self.y)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        k = self.idx\n",
    "        \n",
    "        N = A_tilde_k.shape[0]\n",
    "        \n",
    "        num_class = C_k.shape[-1]\n",
    "        \n",
    "        if (opt == \"Adam\"):\n",
    "            optimizer = optim.Adam(self.model.parameters())\n",
    "            \n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            for X_B, y_B in self.dataloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                B = X_B.shape[0]\n",
    "            \n",
    "                H_B = self.model(X_B)\n",
    "                Z_B = A_tilde_k[k]*H_B + C_k\n",
    "                y_B_hat = F.softmax(Z_B, dim=1)\n",
    "                \n",
    "                if (gradient == True and dH != None):\n",
    "                    \n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"sum\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        y_B_onehot = torch.zeros(B, num_class)\n",
    "                        y_B_onehot[np.arange(B), y_B] = 1\n",
    "                        Errs = y_B_hat - y_B_onehot\n",
    "                        for pname, param in self.model.named_parameters():\n",
    "                            for i in range(N):\n",
    "                                if (i != k):\n",
    "                                    param.grad.data += A_tilde_k[i]*torch.tensordot(Errs, dH[i][pname], dims=1).sum(dim=0)\n",
    "                            param.grad.data = param.grad.data/B\n",
    "                            \n",
    "                else:\n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"mean\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                    \n",
    "                # Gradient Clipping\n",
    "                            \n",
    "                if (gradient_clipping == None):     \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                elif (type(gradient_clipping) == float or type(gradient_clipping) == int):\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clipping, norm_type=2)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(\"Unkown type of gradient clipping value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Server Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Central_Server:\n",
    "    \n",
    "    def __init__(self, node_list, A_tilde):\n",
    "        \n",
    "        \"\"\"\n",
    "        A_tilde: PageRank matrix\n",
    "        node_list: A list contains objects from Node class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A_tilde = A_tilde\n",
    "        self.node_list = node_list\n",
    "        self.N = len(node_list)\n",
    "        self.central_parameters = None\n",
    "        self.cmodel = None\n",
    "        \n",
    "    def init_central_parameters(self, input_dim, hidden_dim, output_dim, nn_type):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the central server parameter dictonary\n",
    "        \"\"\"\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            self.cmodel = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            self.cmodel = LR(input_dim, output_dim)\n",
    "            \n",
    "        \n",
    "        self.central_parameters = copy.deepcopy(self.cmodel.state_dict())\n",
    "        \n",
    "        \n",
    "    def broadcast_central_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Broadcast the current central parameters to all nodes.\n",
    "        Usually used after the aggregation in the end of each communication\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.central_parameters == None:\n",
    "            raise ValueError(\"Central parameters is None, Please initilalize it first.\")\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            node.receieve_central_parameters(self.central_parameters)\n",
    "        \n",
    "    def collect_hs(self, gradient=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Collect h and dh from each node.\n",
    "        \"\"\"\n",
    "        \n",
    "        H = []\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            dH = []\n",
    "\n",
    "            for i in range(self.N):\n",
    "                h_i, dh_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "                dH.append(dh_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "\n",
    "            # dH: a list of gradient dictonaries\n",
    "            return H, dH\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.N):\n",
    "                h_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "            \n",
    "            return H, None\n",
    "        \n",
    "            \n",
    "    def collect_data(self, m):\n",
    "        \n",
    "        Xs = []\n",
    "        \n",
    "        ys = []\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            \n",
    "            X, y = node.upload_data(m)\n",
    "            \n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "            \n",
    "            \n",
    "        # Xs; [m, N, p]\n",
    "        # ys: [m, N]\n",
    "            \n",
    "        Xs = torch.cat(Xs, dim=1)\n",
    "        \n",
    "        ys = torch.cat(ys, dim=1)\n",
    "        \n",
    "        return Xs, ys\n",
    "            \n",
    "            \n",
    "        \n",
    "    def communication(self, train_indices, test_indices, I, \n",
    "                      aggregation=mean_agg, \n",
    "                      opt=\"Adam\", learning_rate=0.1, \n",
    "                      num_epochs=10, gradient=True, m=10, \n",
    "                      gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        train_indices: A list of indices for the nodes that will be used during training.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        test_indices: A list of indices for the nodes that will be used for testing purpose.\n",
    "        \n",
    "        num_epochs: Number of training epochs for each training node during local update.\n",
    "        \n",
    "        aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "        \n",
    "        learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "        opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "\n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "\n",
    "        m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "           Default: 10\n",
    "           \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                           if a number (int or float) is given, then the maximum norm is determined by this number. \n",
    "                           Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.broadcast_central_parameters()\n",
    "        \n",
    "        # H: [N, num_class]\n",
    "        H, dH = self.collect_hs(gradient)\n",
    "        \n",
    "        # C: [N, num_class]\n",
    "        with torch.no_grad():\n",
    "            C = torch.matmul(self.A_tilde, H)\n",
    "        \n",
    "        for k in train_indices:\n",
    "            with torch.no_grad():\n",
    "                C_k = C[k,:] - self.A_tilde[k,k]*H[k,:]\n",
    "    \n",
    "            self.node_list[k].local_update(self.A_tilde[k,:], C_k, dH, \n",
    "                                           I, opt, learning_rate, num_epochs, gradient, gradient_clipping)\n",
    "            \n",
    "        aggregation(self.central_parameters, self.cmodel, self.node_list, train_indices)\n",
    "        \n",
    "        \n",
    "        # Xs: [m, N, p]\n",
    "        # ys: [m, N]\n",
    "        Xs, ys = self.collect_data(m)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Hs: [m, N, num_class]\n",
    "            Hs = self.cmodel(Xs)\n",
    "            \n",
    "            # Zs: [m, N, num_class]\n",
    "            Zs = torch.matmul(self.A_tilde, Hs)\n",
    "            \n",
    "            \n",
    "            # train_Zs: [m, num_train, num_class]\n",
    "            # train_ys: [m, num_train]\n",
    "            train_Zs = Zs[:,train_indices,:]\n",
    "            train_ys = ys[:,train_indices]\n",
    "            \n",
    "            num_train = len(train_indices)\n",
    "            \n",
    "            train_loss = F.cross_entropy(train_Zs.view(m*num_train, -1), train_ys.view(m*num_train)).item()\n",
    "        \n",
    "        return train_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CSBM(csbm, A_tilde, hidden_dim, num_train, I, \n",
    "               num_communication=20, aggregation=mean_agg,\n",
    "               learning_rate=0.1, opt=\"Adam\", num_epochs=10,\n",
    "               gradient=True, m=10, gradient_clipping=None,\n",
    "               nn_type=\"MLP\", output_dim=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    csbm: An cSBM object (contextual stochastic block model)\n",
    "    \n",
    "    A_tilde: pageRank matrix\n",
    "    \n",
    "    I: number of local updates for each node, so batch size = n_k/I for each node k.\n",
    "    \n",
    "    num_train: Number of nodes used in training.\n",
    "    \n",
    "    aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "    \n",
    "    num_communication: Number of communicatons. Default: 20\n",
    "    \n",
    "    learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "    opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "    \n",
    "    gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "    \n",
    "    m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "       Default: 10\n",
    "       \n",
    "    gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                       if a number (int or float) is given, \n",
    "                       then the maximum norm is determined by this number. Default: None.\n",
    "                       \n",
    "    nn_type: The type of neural network. either \"MLP\" or \"LR\" (i.e. MLP or Logistic Regression). Default:\"MLP\".\n",
    "    \"\"\"\n",
    "    \n",
    "    N = A_tilde.shape[0]\n",
    "    \n",
    "    input_dim = csbm.p\n",
    "    \n",
    "    node_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            model_i = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            model_i = LR(input_dim, output_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Type of neural network must be either LR or MLP!\")\n",
    "        \n",
    "        node_i = Node(local_model=model_i, node_idx=i, X=csbm.Xs[i], y=csbm.ys[i])\n",
    "        \n",
    "        node_list.append(node_i)\n",
    "        \n",
    "    server = Central_Server(node_list, A_tilde)\n",
    "    \n",
    "    server.init_central_parameters(input_dim, hidden_dim, output_dim, nn_type)\n",
    "    \n",
    "    if (num_train == N):\n",
    "        \n",
    "        train_indices = np.arange(N)\n",
    "        \n",
    "        test_indices = []\n",
    "        \n",
    "        np.random.shuffle(train_indices)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        class1_train = np.random.choice(a=csbm.class1_ids, size=int(num_train/2), replace=False)\n",
    "\n",
    "        class2_train = np.random.choice(a=csbm.class2_ids, size=int(num_train/2), replace=False)\n",
    "\n",
    "        train_indices = np.concatenate((class1_train, class2_train), axis=0)\n",
    "\n",
    "        test_indices = list(set(np.arange(N)) - set(train_indices))\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for ith in range(num_communication):\n",
    "        \n",
    "        average_train_loss = server.communication(train_indices, test_indices, \n",
    "                                                  I, aggregation, opt, learning_rate, num_epochs,\n",
    "                                                  gradient, m, gradient_clipping)\n",
    "        train_loss.append(average_train_loss)\n",
    "        \n",
    "        if (num_communication <= 30):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication > 30 and num_communication <= 100):\n",
    "            if (ith % 5 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication >= 10000):\n",
    "            if (ith % 100 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        else:\n",
    "            if (ith % 10 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = 10\n",
    "d = 5\n",
    "mu = 1\n",
    "l = 2\n",
    "csbm = cSBM(N, p, d, mu, l)\n",
    "A_tilde = calculate_Atilde(csbm.A, 100, 0.95)\n",
    "\n",
    "csbm.generate_features(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1 Average train loss: 0.6901199221611023\n",
      "Communication: 101 Average train loss: 0.6866954565048218\n",
      "Communication: 201 Average train loss: 0.6817219853401184\n",
      "Communication: 301 Average train loss: 0.678386390209198\n",
      "Communication: 401 Average train loss: 0.6735882759094238\n",
      "Communication: 501 Average train loss: 0.6714518666267395\n",
      "Communication: 601 Average train loss: 0.6681585907936096\n"
     ]
    }
   ],
   "source": [
    "tl = train_CSBM(csbm=csbm, A_tilde=A_tilde, hidden_dim=200,\n",
    "           I=1, num_communication=10000, aggregation=mean_agg, num_train=100, \n",
    "           num_epochs=1,\n",
    "           gradient=True, m=20, gradient_clipping=None, nn_type=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20dcb078c10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
