{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import copy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bias=False):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        X: [batch_size, input_dim], float tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        if (X.type() != 'torch.FloatTensor'):\n",
    "            X = X.type(torch.FloatTensor)\n",
    "        \n",
    "        X = F.relu(self.linear1(X))\n",
    "        H = self.linear2(X)\n",
    "        \n",
    "        # H: [batch_size, output_dim], the feature representation\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, bias=False):\n",
    "        \n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=bias)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        X: [batch_size, input_dim], float tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        if (X.type() != 'torch.FloatTensor'):\n",
    "            X = X.type(torch.FloatTensor)\n",
    "        \n",
    "        H = self.linear(X)\n",
    "        \n",
    "        # H: [batch_size, output_dim], the feature representation\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A_tilde Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Atilde(A, K, alpha):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    A: adjacent matrix, numpy array, [N, N]\n",
    "    K: number of power iterations, scalar\n",
    "    alpha: jump probability, scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Number of nodes in this graph\n",
    "    N = A.shape[0]\n",
    "    \n",
    "    # Add a self loop\n",
    "    A = A + np.identity(N)\n",
    "    \n",
    "    # Update the degree matrix (Because the added self-loops)\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    \n",
    "    # Calculate A_hat and (D^{1/2})^{-1}\n",
    "    D_sqrt_inv = scipy.linalg.inv(scipy.linalg.sqrtm(D))\n",
    "    A_hat = D_sqrt_inv @ A @ D_sqrt_inv\n",
    "    \n",
    "    \n",
    "    # Power iteration: A_tilde = (1-\\alpha)(\\sum_{i=0}^{K} \\alpha^{i}\\hat{A}^{i})\n",
    "    A_tilde = np.zeros((N,N))\n",
    "    A_hat_i = np.identity(N)\n",
    "    alpha_i = 1\n",
    "    for i in range(0, K+1):\n",
    "        A_tilde = A_tilde + alpha_i*A_hat_i\n",
    "        alpha_i = alpha_i*alpha\n",
    "        A_hat_i = A_hat_i @ A_hat\n",
    "    A_tilde = (1-alpha)*A_tilde\n",
    "    \n",
    "    # A_tilde: [N, N], 2-d float tensor\n",
    "    return torch.tensor(A_tilde).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cSBM:\n",
    "    \n",
    "    def __init__(self, N, p, d, mu, l):\n",
    "        \n",
    "        \"\"\"\n",
    "        N: number of nodes\n",
    "        p: dimension of feature vector \n",
    "        d: average degree\n",
    "        l: lambda, hyperparameter\n",
    "        mu: mu, hyperparameter\n",
    "        \n",
    "        For details: https://arxiv.org/pdf/1807.09596.pdf\n",
    "        and https://openreview.net/pdf/3fd51494885a4f0252dd144ae51025065fef2186.pdf\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Generate class from {-1, 1} for each node\n",
    "        v = np.random.choice(a = [-1, 1],\n",
    "                             size = N,\n",
    "                             replace = True,\n",
    "                             p = [0.5, 0.5])\n",
    "        \n",
    "        class1_ids = np.argwhere(v==1)\n",
    "        \n",
    "        class2_ids = np.argwhere(v==-1)\n",
    "        \n",
    "        \n",
    "        # Mask -1 to 0 and store the result in v_mask\n",
    "        v_mask = np.copy(v)\n",
    "        v_mask[v==-1] = 0\n",
    "        \n",
    "        # calculate c_in and c_out\n",
    "        c_in = d + np.sqrt(d)*l\n",
    "        c_out = d - np.sqrt(d)*l\n",
    "        \n",
    "        \n",
    "        # Generate a latent random vector u with size p\n",
    "        u = np.random.normal(loc=0, scale=1/np.sqrt(p), size=p)\n",
    "        \n",
    "        # Generate the adjacent matrix without self-loop\n",
    "        A = np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(i+1, N):\n",
    "                if (v[i] == v[j]):\n",
    "                    if (np.random.choice(a = [1,0],p = [c_in/N, 1-c_in/N])):\n",
    "                        A[i,j] = 1.0\n",
    "                    else:\n",
    "                        A[i,j] = 0.0\n",
    "                else:\n",
    "                    if (np.random.choice(a = [1,0],p = [c_out/N, 1-c_out/N])):\n",
    "                        A[i,j] = 1.0\n",
    "                    else:\n",
    "                        A[i,j] = 0.0\n",
    "        A = A + A.T\n",
    "        \n",
    "        # Save all the necessary parameters\n",
    "        self.v = v\n",
    "        self.v_mask = v_mask\n",
    "        self.A = A\n",
    "        self.u = u\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        self.mu = mu\n",
    "        xi = N/p\n",
    "        self.phi = np.arctan((l*np.sqrt(xi))/mu)*(2/np.pi)\n",
    "        self.threshold = l**2 + (mu**2)/(N/p)\n",
    "        self.class1_ids = class1_ids.reshape(-1)\n",
    "        self.class2_ids = class2_ids.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_agg(central_parameters, central_model, node_list, train_indices):\n",
    "    \n",
    "    num_train = len(train_indices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pname, param in central_model.named_parameters():\n",
    "            \n",
    "            p = node_list[train_indices[0]].model.state_dict()[pname]\n",
    "            \n",
    "            for i in range(1, num_train):\n",
    "                \n",
    "                p = p + node_list[train_indices[i]].model.state_dict()[pname]\n",
    "            \n",
    "            p = p/num_train\n",
    "            \n",
    "            central_parameters[pname] = p\n",
    "            \n",
    "            param.copy_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \n",
    "    def __init__(self, local_model, node_idx, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        local model: The local MLP model for each node\n",
    "        node_idx: The unique index of a node\n",
    "        X: [n_k, p], feature matrix, float tensor\n",
    "        y: [n_k], true labels, long tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = local_model\n",
    "        self.idx = node_idx\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_k = X.shape[0]\n",
    "        self.dataloader = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        \n",
    "    def upload_local_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Upload local model parameters to central server.\n",
    "        Usually used for aggregation step in each communication.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    \n",
    "    def receieve_central_parameters(self, central_parameters):\n",
    "        \n",
    "        \"\"\"\n",
    "        central_parameters: A state dictonary for central server parameters.\n",
    "        \n",
    "        Receive the broadcasted central parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pname, param in self.model.named_parameters():\n",
    "                param.copy_(central_parameters[pname])\n",
    "                \n",
    "                \n",
    "    def upload_h(self, gradient=True):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        This function uploads an random hidden vector from a node to the central server.\n",
    "        It also calculate and upload a dictonary of gradients  (dh/dw, 3D tensors) for each parameter w.r.t the local model\n",
    "        \"\"\" \n",
    "        \n",
    "        # x: [p]\n",
    "        x = self.X[np.random.choice(a=self.n_k),:]\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            # Clear the possible accumulated gradient of the parameters of local model\n",
    "            self.model.zero_grad()\n",
    "        \n",
    "            h = self.model(x).view(1, -1)\n",
    "            \n",
    "            num_class = h.shape[-1]\n",
    "\n",
    "            dh = {}\n",
    "\n",
    "            for i in range(num_class):\n",
    "\n",
    "                h[0, i].backward(retain_graph=True)\n",
    "\n",
    "                for pname, param in self.model.named_parameters():\n",
    "\n",
    "                    if pname in dh:\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "                    else:\n",
    "                        dh[pname] = []\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "\n",
    "                    if (i == num_class-1):\n",
    "                        d1, d2 = dh[pname][0].shape\n",
    "                        dh[pname] = torch.cat(dh[pname], dim=0).view(num_class, d1, d2)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            return h, dh\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                h = self.model(x).view(1, -1)\n",
    "                \n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def upload_data(self, m=1):\n",
    "        \n",
    "        if (m > self.n_k):\n",
    "            raise ValueError(\"m is bigger than n_k!\")\n",
    "            \n",
    "        \n",
    "        ids = np.random.choice(a=self.n_k, size=m, replace=False)\n",
    "        \n",
    "        X = self.X[ids,:].view(m, 1, -1)\n",
    "        \n",
    "        y = self.y[ids].view(m, 1)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def local_update(self, A_tilde_k, C_k, dH, I, \n",
    "                     opt=\"Adam\",\n",
    "                     learning_rate=0.01, num_epochs=10, \n",
    "                     gradient=True, gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        The local update process for a node k.\n",
    "        \n",
    "        A_tilde_k: The kth row of PageRank matrix A_tilde.\n",
    "        \n",
    "        C_k: [1, num_class] The aggregated neighborhood information for node k.\n",
    "        \n",
    "        dH: A list of gradient dictonaries, where the kth dictonary contains the gradients of each parameter for node k.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        opt: Optimizer used for local updates: SGD or Adam. Default: \"Adam\"\n",
    "        \n",
    "        learning rate: learning rate for SGD. Default: 0.1\n",
    "        \n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "        \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "        if a number (int or float) is given, then the maximum norm is determined by this number. Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (self.dataloader == None):\n",
    "            batch_size = int(np.floor(self.n_k/I))\n",
    "            dataset = torch.utils.data.TensorDataset(self.X, self.y)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        k = self.idx\n",
    "        \n",
    "        N = A_tilde_k.shape[0]\n",
    "        \n",
    "        num_class = C_k.shape[-1]\n",
    "        \n",
    "        if (opt == \"Adam\"):\n",
    "            optimizer = optim.Adam(self.model.parameters())\n",
    "            \n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            for X_B, y_B in self.dataloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                B = X_B.shape[0]\n",
    "            \n",
    "                H_B = self.model(X_B)\n",
    "                y_B_onehot = torch.zeros(B, num_class)\n",
    "                y_B_onehot[np.arange(B), y_B] = 1\n",
    "\n",
    "                Z_B = A_tilde_k[k]*H_B + C_k\n",
    "                y_B_hat = F.softmax(Z_B, dim=1)\n",
    "                \n",
    "                if (gradient == True and dH != None):\n",
    "                    \n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"sum\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        Errs = y_B_hat - y_B_onehot\n",
    "                        for pname, param in self.model.named_parameters():\n",
    "                            for i in range(N):\n",
    "                                if (i != k):\n",
    "                                    param.grad.data += A_tilde_k[i]*torch.tensordot(Errs, dH[i][pname], dims=1).sum(dim=0)\n",
    "                            param.grad.data = param.grad.data/B\n",
    "                            \n",
    "                else:\n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"mean\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                            \n",
    "                if (gradient_clipping == None):     \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                elif (type(gradient_clipping) == float or type(gradient_clipping) == int):\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clipping, norm_type=2)\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    raise ValueError(\"Unkown type of gradient clipping value!\")\n",
    "                    \n",
    "                \n",
    "    def local_eval(self, central_model, A_tilde_kk, C_k):\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            H = central_model(self.X)\n",
    "            y_hat = F.softmax(A_tilde_kk*H+C_k, dim=1)\n",
    "            loss = F.nll_loss(torch.log(y_hat), self.y, reduction=\"mean\")\n",
    "            preds = torch.max(y_hat, dim=1)[1]\n",
    "            count = (preds == self.y).sum().item()\n",
    "            \n",
    "        return loss.item(), count/self.n_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Server Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Central_Server:\n",
    "    \n",
    "    def __init__(self, node_list, A_tilde):\n",
    "        \n",
    "        \"\"\"\n",
    "        A_tilde: PageRank matrix\n",
    "        node_list: A list contains objects from Node class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A_tilde = A_tilde\n",
    "        self.node_list = node_list\n",
    "        self.N = len(node_list)\n",
    "        self.central_parameters = None\n",
    "        self.cmodel = None\n",
    "        \n",
    "    def init_central_parameters(self, input_dim, hidden_dim, output_dim, nn_type):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the central server parameter dictonary\n",
    "        \"\"\"\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            self.cmodel = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            self.cmodel = LR(input_dim, output_dim)\n",
    "            \n",
    "        \n",
    "        self.central_parameters = copy.deepcopy(self.cmodel.state_dict())\n",
    "        \n",
    "        \n",
    "    def broadcast_central_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Broadcast the current central parameters to all nodes.\n",
    "        Usually used after the aggregation in the end of each communication\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.central_parameters == None:\n",
    "            raise ValueError(\"Central parameters is None, Please initilalize it first.\")\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            node.receieve_central_parameters(self.central_parameters)\n",
    "        \n",
    "    def collect_hs(self, gradient=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Collect h and dh from each node.\n",
    "        \"\"\"\n",
    "        \n",
    "        H = []\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            dH = []\n",
    "\n",
    "            for i in range(self.N):\n",
    "                h_i, dh_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "                dH.append(dh_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "\n",
    "            # dH: a list of gradient dictonaries\n",
    "            return H, dH\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.N):\n",
    "                h_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "            \n",
    "            return H, None\n",
    "        \n",
    "            \n",
    "    def collect_data(self, m):\n",
    "        \n",
    "        Xs = []\n",
    "        \n",
    "        ys = []\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            \n",
    "            X, y = node.upload_data(m)\n",
    "            \n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "            \n",
    "            \n",
    "        # Xs; [m, N, p]\n",
    "        # ys: [m, N]\n",
    "            \n",
    "        Xs = torch.cat(Xs, dim=1)\n",
    "        \n",
    "        ys = torch.cat(ys, dim=1)\n",
    "        \n",
    "        return Xs, ys\n",
    "            \n",
    "            \n",
    "        \n",
    "    def communication(self, train_indices, test_indices, I, \n",
    "                      aggregation=mean_agg, \n",
    "                      opt=\"Adam\", learning_rate=0.1, \n",
    "                      num_epochs=10, gradient=True, m=10, \n",
    "                      gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        train_indices: A list of indices for the nodes that will be used during training.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        test_indices: A list of indices for the nodes that will be used for testing purpose.\n",
    "        \n",
    "        num_epochs: Number of training epochs for each training node during local update.\n",
    "        \n",
    "        aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "        \n",
    "        learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "        opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "\n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "\n",
    "        m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "           Default: 10\n",
    "           \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                           if a number (int or float) is given, then the maximum norm is determined by this number. \n",
    "                           Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.broadcast_central_parameters()\n",
    "        \n",
    "        # H: [N, num_class]\n",
    "        H, dH = self.collect_hs(gradient)\n",
    "        \n",
    "        # C: [N, num_class]\n",
    "        with torch.no_grad():\n",
    "            C = torch.matmul(self.A_tilde, H)\n",
    "        \n",
    "        for k in train_indices:\n",
    "            with torch.no_grad():\n",
    "                C_k = C[k,:] - self.A_tilde[k,k]*H[k,:]\n",
    "    \n",
    "            self.node_list[k].local_update(self.A_tilde[k,:], C_k, dH, \n",
    "                                           I, opt, learning_rate, num_epochs, gradient, gradient_clipping)\n",
    "            \n",
    "        aggregation(self.central_parameters, self.cmodel, self.node_list, train_indices)\n",
    "        \n",
    "        \n",
    "        # Xs: [m, N, p]\n",
    "        # ys: [m, N]\n",
    "        Xs, ys = self.collect_data(m)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Hs: [m, N, num_class]\n",
    "            Hs = self.cmodel(Xs)\n",
    "            \n",
    "            # Zs: [m, N, num_class]\n",
    "            Zs = torch.matmul(self.A_tilde, Hs)\n",
    "            \n",
    "            \n",
    "            # train_Zs: [m, num_train, num_class]\n",
    "            # train_ys: [m, num_train]\n",
    "            train_Zs = Zs[:,train_indices,:]\n",
    "            train_ys = ys[:,train_indices]\n",
    "            \n",
    "            num_train = len(train_indices)\n",
    "            \n",
    "            train_loss = F.cross_entropy(train_Zs.view(m*num_train, -1), train_ys.view(m*num_train)).item()\n",
    "        \n",
    "        return train_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CSBM(csbm, A_tilde, hidden_dim, num_train, I, n_k, \n",
    "               num_communication=20, aggregation=mean_agg,\n",
    "               learning_rate=0.1, opt=\"Adam\", num_epochs=10,\n",
    "               gradient=True, m=10, gradient_clipping=None,\n",
    "               nn_type=\"MLP\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    csbm: An cSBM object (contextual stochastic block model)\n",
    "    \n",
    "    A_tilde: pageRank matrix\n",
    "    \n",
    "    n_k: number of feature vectors each node has.\n",
    "    \n",
    "    I: number of local updates for each node, so batch size = n_k/I for each node k.\n",
    "    \n",
    "    num_train: Number of nodes used in training.\n",
    "    \n",
    "    aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "    \n",
    "    num_communication: Number of communicatons. Default: 20\n",
    "    \n",
    "    learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "    opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "    \n",
    "    gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "    \n",
    "    m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "       Default: 10\n",
    "       \n",
    "    gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                       if a number (int or float) is given, \n",
    "                       then the maximum norm is determined by this number. Default: None.\n",
    "                       \n",
    "    nn_type: The type of neural network. either \"MLP\" or \"LR\" (i.e. MLP or Logistic Regression). Default:\"MLP\".\n",
    "    \"\"\"\n",
    "    \n",
    "    N = A_tilde.shape[0]\n",
    "    \n",
    "    input_dim = csbm.p\n",
    "    \n",
    "    output_dim = 2\n",
    "    \n",
    "    node_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            model_i = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            model_i = LR(input_dim, output_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Type of neural network must be either LR or MLP!\")\n",
    "            \n",
    "        \n",
    "        for j in range(n_k):\n",
    "            \n",
    "            x_j = np.sqrt(csbm.mu/N)*csbm.v[i]*csbm.u + np.random.normal(loc=0, scale=1, size=csbm.p)/np.sqrt(csbm.p)\n",
    "            \n",
    "            X.append(x_j)\n",
    "            \n",
    "        X = torch.tensor(np.array(X))\n",
    "        \n",
    "        if csbm.v[i] == -1:\n",
    "            \n",
    "            y = np.zeros(n_k)\n",
    "            \n",
    "        elif csbm.v[i] == 1:\n",
    "            \n",
    "            y = np.ones(n_k)\n",
    "\n",
    "        y = torch.tensor(y).type(torch.LongTensor)\n",
    "        \n",
    "        node_i = Node(local_model=model_i, node_idx=i, X=X, y=y)\n",
    "        \n",
    "        node_list.append(node_i)\n",
    "        \n",
    "    server = Central_Server(node_list, A_tilde)\n",
    "    \n",
    "    server.init_central_parameters(input_dim, hidden_dim, output_dim, nn_type)\n",
    "    \n",
    "    class1_train = np.random.choice(a=csbm.class1_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    class2_train = np.random.choice(a=csbm.class2_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    train_indices = np.concatenate((class1_train, class2_train), axis=0)\n",
    "    \n",
    "    test_indices = list(set(np.arange(N)) - set(train_indices))\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for ith in range(num_communication):\n",
    "        \n",
    "        average_train_loss = server.communication(train_indices, test_indices, \n",
    "                                                  I, aggregation, opt, learning_rate, num_epochs,\n",
    "                                                  gradient, m, gradient_clipping)\n",
    "        train_loss.append(average_train_loss)\n",
    "        \n",
    "        if (num_communication <= 30):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication > 30 and num_communication <= 100):\n",
    "            if (ith % 5 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication >= 10000):\n",
    "            if (ith % 100 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        else:\n",
    "            if (ith % 10 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1 Average train loss: 0.6894458532333374\n",
      "Communication: 101 Average train loss: 0.6102525591850281\n",
      "Communication: 201 Average train loss: 0.6007277965545654\n",
      "Communication: 301 Average train loss: 0.5598114132881165\n",
      "Communication: 401 Average train loss: 0.5559687614440918\n",
      "Communication: 501 Average train loss: 0.5679489374160767\n",
      "Communication: 601 Average train loss: 0.5488091111183167\n",
      "Communication: 701 Average train loss: 0.5407792329788208\n",
      "Communication: 801 Average train loss: 0.5965477824211121\n",
      "Communication: 901 Average train loss: 0.5846758484840393\n",
      "Communication: 1001 Average train loss: 0.6462271809577942\n",
      "Communication: 1101 Average train loss: 0.5658426284790039\n",
      "Communication: 1201 Average train loss: 0.6196436285972595\n",
      "Communication: 1301 Average train loss: 0.5758956670761108\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2d0d0d402531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m            \u001b[0mI\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_communication\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m            \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m            gradient=False, m=20, gradient_clipping=None, nn_type=\"LR\")\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-c3f5af7dfb3c>\u001b[0m in \u001b[0;36mtrain_CSBM\u001b[1;34m(csbm, A_tilde, hidden_dim, num_train, I, n_k, num_communication, aggregation, learning_rate, opt, num_epochs, gradient, m, gradient_clipping, nn_type)\u001b[0m\n\u001b[0;32m     98\u001b[0m         average_train_loss = server.communication(train_indices, test_indices, \n\u001b[0;32m     99\u001b[0m                                                   \u001b[0mI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                                                   gradient, m, gradient_clipping)\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_train_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6e790d417627>\u001b[0m in \u001b[0;36mcommunication\u001b[1;34m(self, train_indices, test_indices, I, aggregation, opt, learning_rate, num_epochs, gradient, m, gradient_clipping)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             self.node_list[k].local_update(self.A_tilde[k,:], C_k, dH, \n\u001b[1;32m--> 149\u001b[1;33m                                            I, opt, learning_rate, num_epochs, gradient, gradient_clipping)\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcentral_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-472420871d3d>\u001b[0m in \u001b[0;36mlocal_update\u001b[1;34m(self, A_tilde_k, C_k, dH, I, opt, learning_rate, num_epochs, gradient, gradient_clipping)\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_B\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                 \u001b[0mH_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[0my_B_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0my_B_onehot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_B\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Machine Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-2fb7e2ed0ad5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'torch.FloatTensor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tl = train_CSBM(csbm=csbm, A_tilde=A_tilde, hidden_dim=200, n_k=40,\n",
    "           I=4, num_communication=10000, aggregation=mean_agg, num_train=10, \n",
    "           num_epochs=20,\n",
    "           gradient=False, m=20, gradient_clipping=None, nn_type=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = 10\n",
    "d = 5\n",
    "mu = 1\n",
    "l = 2\n",
    "csbm = cSBM(N, p, d, mu, l)\n",
    "A_tilde = calculate_Atilde(csbm.A, 100, 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
