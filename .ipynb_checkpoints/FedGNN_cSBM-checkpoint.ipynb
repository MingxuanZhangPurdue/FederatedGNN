{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import copy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils import calculate_Atilde, cSBM, mean_agg, MLP, LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \n",
    "    def __init__(self, local_model, node_idx, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        local model: The local MLP model for each node\n",
    "        node_idx: The unique index of a node\n",
    "        X: [n_k, p], feature matrix, float tensor\n",
    "        y: [n_k], true labels, long tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = local_model\n",
    "        self.idx = node_idx\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_k = X.shape[0]\n",
    "        self.dataloader = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        \n",
    "    def upload_local_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Upload local model parameters to central server.\n",
    "        Usually used for aggregation step in each communication.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    \n",
    "    def receieve_central_parameters(self, central_parameters):\n",
    "        \n",
    "        \"\"\"\n",
    "        central_parameters: A state dictonary for central server parameters.\n",
    "        \n",
    "        Receive the broadcasted central parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pname, param in self.model.named_parameters():\n",
    "                param.copy_(central_parameters[pname])\n",
    "                \n",
    "                \n",
    "    def upload_h(self, gradient=True):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        This function uploads an random hidden vector from a node to the central server.\n",
    "        It also calculate and upload a dictonary of gradients  (dh/dw, 3D tensors) for each parameter w.r.t the local model\n",
    "        \"\"\" \n",
    "        \n",
    "        x = self.X[np.random.choice(a=self.n_k),:]\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            # Clear the possible accumulated gradient of the parameters of local model\n",
    "            self.model.zero_grad()\n",
    "        \n",
    "            h = self.model(x).view(1, -1)\n",
    "            \n",
    "            num_class = h.shape[-1]\n",
    "\n",
    "            dh = {}\n",
    "\n",
    "            for i in range(num_class):\n",
    "\n",
    "                h[0, i].backward(retain_graph=True)\n",
    "\n",
    "                for pname, param in self.model.named_parameters():\n",
    "\n",
    "                    if pname in dh:\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "                    else:\n",
    "                        dh[pname] = []\n",
    "                        dh[pname].append(param.grad.data.clone())\n",
    "\n",
    "                    if (i == num_class-1):\n",
    "                        d1, d2 = dh[pname][0].shape\n",
    "                        dh[pname] = torch.cat(dh[pname], dim=0).view(num_class, d1, d2)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            return h, dh\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                h = self.model(x).view(1, -1)\n",
    "                \n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def upload_data(self, m=1):\n",
    "        \n",
    "        # Upload the m number of local data for evaluation purpose.\n",
    "        \n",
    "        if (m > self.n_k):\n",
    "            raise ValueError(\"m is bigger than n_k!\")\n",
    "            \n",
    "        ids = np.random.choice(a=self.n_k, size=m, replace=False)\n",
    "        \n",
    "        X = self.X[ids,:].view(m, 1, -1)\n",
    "        \n",
    "        y = self.y[ids].view(m, 1)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def local_update(self, A_tilde_k, C_k, dH, I, \n",
    "                     opt=\"Adam\",\n",
    "                     learning_rate=0.01, num_epochs=10, \n",
    "                     gradient=True, gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        The local update process for a node k.\n",
    "        \n",
    "        A_tilde_k: The kth row of PageRank matrix A_tilde.\n",
    "        \n",
    "        C_k: [1, num_class] The aggregated neighborhood information for node k.\n",
    "        \n",
    "        dH: A list of gradient dictonaries, where the kth dictonary contains the gradients of each parameter for node k.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        opt: Optimizer used for local updates: SGD or Adam. Default: \"Adam\"\n",
    "        \n",
    "        learning rate: learning rate for SGD. Default: 0.1\n",
    "        \n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "        \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "        if a number (int or float) is given, then the maximum norm is determined by this number. Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (self.dataloader == None):\n",
    "            batch_size = int(np.floor(self.n_k/I))\n",
    "            dataset = torch.utils.data.TensorDataset(self.X, self.y)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        k = self.idx\n",
    "        \n",
    "        N = A_tilde_k.shape[0]\n",
    "        \n",
    "        num_class = C_k.shape[-1]\n",
    "        \n",
    "        if (opt == \"Adam\"):\n",
    "            optimizer = optim.Adam(self.model.parameters())\n",
    "            \n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            for X_B, y_B in self.dataloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                B = X_B.shape[0]\n",
    "            \n",
    "                H_B = self.model(X_B)\n",
    "                Z_B = A_tilde_k[k]*H_B + C_k\n",
    "                y_B_hat = F.softmax(Z_B, dim=1)\n",
    "                \n",
    "                if (gradient == True and dH != None):\n",
    "                    \n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"sum\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        y_B_onehot = torch.zeros(B, num_class)\n",
    "                        y_B_onehot[np.arange(B), y_B] = 1\n",
    "                        Errs = y_B_hat - y_B_onehot\n",
    "                        for pname, param in self.model.named_parameters():\n",
    "                            for i in range(N):\n",
    "                                if (i != k):\n",
    "                                    param.grad.data += A_tilde_k[i]*torch.tensordot(Errs, dH[i][pname], dims=1).sum(dim=0)\n",
    "                            param.grad.data = param.grad.data/B\n",
    "                            \n",
    "                else:\n",
    "                    batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"mean\")\n",
    "                    batch_loss.backward()\n",
    "                    \n",
    "                    \n",
    "                # Gradient Clipping\n",
    "                            \n",
    "                if (gradient_clipping == None):     \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                elif (type(gradient_clipping) == float or type(gradient_clipping) == int):\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clipping, norm_type=2)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(\"Unkown type of gradient clipping value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Server Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Central_Server:\n",
    "    \n",
    "    def __init__(self, node_list, A_tilde):\n",
    "        \n",
    "        \"\"\"\n",
    "        A_tilde: PageRank matrix\n",
    "        node_list: A list contains objects from Node class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A_tilde = A_tilde\n",
    "        self.node_list = node_list\n",
    "        self.N = len(node_list)\n",
    "        self.central_parameters = None\n",
    "        self.cmodel = None\n",
    "        \n",
    "    def init_central_parameters(self, input_dim, hidden_dim, output_dim, nn_type):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the central server parameter dictonary\n",
    "        \"\"\"\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            self.cmodel = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            self.cmodel = LR(input_dim, output_dim)\n",
    "            \n",
    "        \n",
    "        self.central_parameters = copy.deepcopy(self.cmodel.state_dict())\n",
    "        \n",
    "        \n",
    "    def broadcast_central_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Broadcast the current central parameters to all nodes.\n",
    "        Usually used after the aggregation in the end of each communication\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.central_parameters == None:\n",
    "            raise ValueError(\"Central parameters is None, Please initilalize it first.\")\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            node.receieve_central_parameters(self.central_parameters)\n",
    "        \n",
    "    def collect_hs(self, gradient=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Collect h and dh from each node.\n",
    "        \"\"\"\n",
    "        \n",
    "        H = []\n",
    "        \n",
    "        if gradient:\n",
    "            \n",
    "            dH = []\n",
    "\n",
    "            for i in range(self.N):\n",
    "                h_i, dh_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "                dH.append(dh_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "\n",
    "            # dH: a list of gradient dictonaries\n",
    "            return H, dH\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.N):\n",
    "                h_i = self.node_list[i].upload_h(gradient)\n",
    "                H.append(h_i)\n",
    "\n",
    "            # H: [N, num_class]\n",
    "            H = torch.cat(H, dim=0)\n",
    "            \n",
    "            return H, None\n",
    "        \n",
    "            \n",
    "    def collect_data(self, m):\n",
    "        \n",
    "        Xs = []\n",
    "        \n",
    "        ys = []\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            \n",
    "            X, y = node.upload_data(m)\n",
    "            \n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "            \n",
    "            \n",
    "        # Xs; [m, N, p]\n",
    "        # ys: [m, N]\n",
    "            \n",
    "        Xs = torch.cat(Xs, dim=1)\n",
    "        \n",
    "        ys = torch.cat(ys, dim=1)\n",
    "        \n",
    "        return Xs, ys\n",
    "            \n",
    "            \n",
    "        \n",
    "    def communication(self, train_indices, test_indices, I, \n",
    "                      aggregation=mean_agg, \n",
    "                      opt=\"Adam\", learning_rate=0.1, \n",
    "                      num_epochs=10, gradient=True, m=10, \n",
    "                      gradient_clipping=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        train_indices: A list of indices for the nodes that will be used during training.\n",
    "        \n",
    "        I: Number of local updates.\n",
    "        \n",
    "        test_indices: A list of indices for the nodes that will be used for testing purpose.\n",
    "        \n",
    "        num_epochs: Number of training epochs for each training node during local update.\n",
    "        \n",
    "        aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "        \n",
    "        learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "        opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "\n",
    "        gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "\n",
    "        m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "           Default: 10\n",
    "           \n",
    "        gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                           if a number (int or float) is given, then the maximum norm is determined by this number. \n",
    "                           Default: None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.broadcast_central_parameters()\n",
    "        \n",
    "        # H: [N, num_class]\n",
    "        H, dH = self.collect_hs(gradient)\n",
    "        \n",
    "        # C: [N, num_class]\n",
    "        with torch.no_grad():\n",
    "            C = torch.matmul(self.A_tilde, H)\n",
    "        \n",
    "        for k in train_indices:\n",
    "            with torch.no_grad():\n",
    "                C_k = C[k,:] - self.A_tilde[k,k]*H[k,:]\n",
    "    \n",
    "            self.node_list[k].local_update(self.A_tilde[k,:], C_k, dH, \n",
    "                                           I, opt, learning_rate, num_epochs, gradient, gradient_clipping)\n",
    "            \n",
    "        aggregation(self.central_parameters, self.cmodel, self.node_list, train_indices)\n",
    "        \n",
    "        \n",
    "        # Xs: [m, N, p]\n",
    "        # ys: [m, N]\n",
    "        Xs, ys = self.collect_data(m)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Hs: [m, N, num_class]\n",
    "            Hs = self.cmodel(Xs)\n",
    "            \n",
    "            # Zs: [m, N, num_class]\n",
    "            Zs = torch.matmul(self.A_tilde, Hs)\n",
    "            \n",
    "            \n",
    "            # train_Zs: [m, num_train, num_class]\n",
    "            # train_ys: [m, num_train]\n",
    "            train_Zs = Zs[:,train_indices,:]\n",
    "            train_ys = ys[:,train_indices]\n",
    "            \n",
    "            num_train = len(train_indices)\n",
    "            \n",
    "            train_loss = F.cross_entropy(train_Zs.view(m*num_train, -1), train_ys.view(m*num_train)).item()\n",
    "        \n",
    "        return train_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CSBM(csbm, A_tilde, hidden_dim, num_train, I, \n",
    "               num_communication=20, aggregation=mean_agg,\n",
    "               learning_rate=0.1, opt=\"Adam\", num_epochs=10,\n",
    "               gradient=True, m=10, gradient_clipping=None,\n",
    "               nn_type=\"MLP\", output_dim=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    csbm: An cSBM object (contextual stochastic block model)\n",
    "    \n",
    "    A_tilde: pageRank matrix\n",
    "    \n",
    "    I: number of local updates for each node, so batch size = n_k/I for each node k.\n",
    "    \n",
    "    num_train: Number of nodes used in training.\n",
    "    \n",
    "    aggregation: aggregation method, for now, only mean aggregation is implemented. Default: mean_agg. \n",
    "    \n",
    "    num_communication: Number of communicatons. Default: 20\n",
    "    \n",
    "    learning_rate: Learning rate for SGD. Default: 0.1\n",
    "    \n",
    "    opt: optimization method: Adam or SGD. Default: \"Adam\"\n",
    "    \n",
    "    gradient: boolean, whether to include the \"fake gradient\" or not. Default: True\n",
    "    \n",
    "    m: The number of feature vectors used for training loss evaluation in the end of each communication for each node. \n",
    "       Default: 10\n",
    "       \n",
    "    gradient_clipping: Whether to peform gradient clipping method during training process. None means no gradient clipping,\n",
    "                       if a number (int or float) is given, \n",
    "                       then the maximum norm is determined by this number. Default: None.\n",
    "                       \n",
    "    nn_type: The type of neural network. either \"MLP\" or \"LR\" (i.e. MLP or Logistic Regression). Default:\"MLP\".\n",
    "    \"\"\"\n",
    "    \n",
    "    N = A_tilde.shape[0]\n",
    "    \n",
    "    input_dim = csbm.p\n",
    "    \n",
    "    node_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        if (nn_type == \"MLP\"):\n",
    "            model_i = MLP(input_dim, hidden_dim, output_dim)\n",
    "            \n",
    "        elif (nn_type == \"LR\"):\n",
    "            model_i = LR(input_dim, output_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Type of neural network must be either LR or MLP!\")\n",
    "        \n",
    "        node_i = Node(local_model=model_i, node_idx=i, X=csbm.Xs[i], y=csbm.ys[i])\n",
    "        \n",
    "        node_list.append(node_i)\n",
    "        \n",
    "    server = Central_Server(node_list, A_tilde)\n",
    "    \n",
    "    server.init_central_parameters(input_dim, hidden_dim, output_dim, nn_type)\n",
    "    \n",
    "    if (num_train == N):\n",
    "        \n",
    "        train_indices = np.arange(N)\n",
    "        \n",
    "        test_indices = []\n",
    "        \n",
    "        np.random.shuffle(train_indices)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        class1_train = np.random.choice(a=csbm.class1_ids, size=int(num_train/2), replace=False)\n",
    "\n",
    "        class2_train = np.random.choice(a=csbm.class2_ids, size=int(num_train/2), replace=False)\n",
    "\n",
    "        train_indices = np.concatenate((class1_train, class2_train), axis=0)\n",
    "\n",
    "        test_indices = list(set(np.arange(N)) - set(train_indices))\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for ith in range(num_communication):\n",
    "        \n",
    "        average_train_loss = server.communication(train_indices, test_indices, \n",
    "                                                  I, aggregation, opt, learning_rate, num_epochs,\n",
    "                                                  gradient, m, gradient_clipping)\n",
    "        train_loss.append(average_train_loss)\n",
    "        \n",
    "        if (num_communication <= 30):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication > 30 and num_communication <= 100):\n",
    "            if (ith % 5 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        elif (num_communication >= 10000):\n",
    "            if (ith % 100 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "        else:\n",
    "            if (ith % 10 == 0):\n",
    "                print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = 10\n",
    "d = 5\n",
    "mu = 1\n",
    "l = 2\n",
    "csbm = cSBM(N, p, d, mu, l)\n",
    "A_tilde = calculate_Atilde(csbm.A, 100, 0.95)\n",
    "\n",
    "csbm.generate_features(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1 Average train loss: 0.6939384937286377\n",
      "Communication: 11 Average train loss: 0.6748458743095398\n",
      "Communication: 21 Average train loss: 0.6555912494659424\n",
      "Communication: 31 Average train loss: 0.6330430507659912\n"
     ]
    }
   ],
   "source": [
    "tl = train_CSBM(csbm=csbm, A_tilde=A_tilde, hidden_dim=200,\n",
    "           I=2, num_communication=1000, aggregation=mean_agg, num_train=100, \n",
    "           num_epochs=20,\n",
    "           gradient=False, m=20, gradient_clipping=None, nn_type=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
