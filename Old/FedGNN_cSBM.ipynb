{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import copy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bias=False):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        X: [batch_size, input_dim], float tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        if (X.type() != 'torch.FloatTensor'):\n",
    "            X = X.type(torch.FloatTensor)\n",
    "        \n",
    "        X = F.relu(self.linear1(X))\n",
    "        H = self.linear2(X)\n",
    "        \n",
    "        # H: [batch_size, output_dim], the feature representation\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A_tilde Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Atilde(A, K, alpha):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    A: adjacent matrix, numpy array, [N, N]\n",
    "    K: number of power iterations, scalar\n",
    "    alpha: jump probability, scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Number of nodes in this graph\n",
    "    N = A.shape[0]\n",
    "    \n",
    "    # Add a self loop\n",
    "    A = A + np.identity(N)\n",
    "    \n",
    "    # Update the degree matrix (Because the added self-loops)\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    \n",
    "    # Calculate A_hat and (D^{1/2})^{-1}\n",
    "    D_sqrt_inv = scipy.linalg.inv(scipy.linalg.sqrtm(D))\n",
    "    A_hat = D_sqrt_inv @ A @ D_sqrt_inv\n",
    "    \n",
    "    \n",
    "    # Power iteration: A_tilde = (1-\\alpha)(\\sum_{i=0}^{K} \\alpha^{i}\\hat{A}^{i})\n",
    "    A_tilde = np.zeros((N,N))\n",
    "    A_hat_i = np.identity(N)\n",
    "    alpha_i = 1\n",
    "    for i in range(0, K+1):\n",
    "        A_tilde = A_tilde + alpha_i*A_hat_i\n",
    "        alpha_i = alpha_i*alpha\n",
    "        A_hat_i = A_hat_i @ A_hat\n",
    "    A_tilde = (1-alpha)*A_tilde\n",
    "    \n",
    "    # A_tilde: [N, N], 2-d float tensor\n",
    "    return torch.tensor(A_tilde).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cSBM:\n",
    "    \n",
    "    def __init__(self, N, p, d, mu, l):\n",
    "        \n",
    "        \"\"\"\n",
    "        N: number of nodes\n",
    "        p: dimension of feature vector \n",
    "        d: average degree\n",
    "        l: lambda, hyperparameter\n",
    "        mu: mu, hyperparameter\n",
    "        \n",
    "        For details: https://arxiv.org/pdf/1807.09596.pdf\n",
    "        and https://openreview.net/pdf/3fd51494885a4f0252dd144ae51025065fef2186.pdf\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Generate class from {-1, 1} for each node\n",
    "        v = np.random.choice(a = [-1, 1],\n",
    "                             size = N,\n",
    "                             replace = True,\n",
    "                             p = [0.5, 0.5])\n",
    "        \n",
    "        class1_ids = np.argwhere(v==1)\n",
    "        \n",
    "        class2_ids = np.argwhere(v==-1)\n",
    "        \n",
    "        \n",
    "        # Mask -1 to 0 and store the result in v_mask\n",
    "        v_mask = np.copy(v)\n",
    "        v_mask[v==-1] = 0\n",
    "        \n",
    "        # calculate c_in and c_out\n",
    "        c_in = d + np.sqrt(d)*l\n",
    "        c_out = d - np.sqrt(d)*l\n",
    "        \n",
    "        \n",
    "        # Generate a latent random vector u with size p\n",
    "        u = np.random.normal(loc=0, scale=1/np.sqrt(p), size=p)\n",
    "        \n",
    "        # Generate the adjacent matrix without self-loop\n",
    "        A = np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(i+1, N):\n",
    "                if (v[i] == v[j]):\n",
    "                    if (np.random.choice(a = [1,0],p = [c_in/N, 1-c_in/N])):\n",
    "                        A[i,j] = 1.0\n",
    "                    else:\n",
    "                        A[i,j] = 0.0\n",
    "                else:\n",
    "                    if (np.random.choice(a = [1,0],p = [c_out/N, 1-c_out/N])):\n",
    "                        A[i,j] = 1.0\n",
    "                    else:\n",
    "                        A[i,j] = 0.0\n",
    "        A = A + A.T\n",
    "        \n",
    "        # Save all the necessary parameters\n",
    "        self.v = v\n",
    "        self.v_mask = v_mask\n",
    "        self.A = A\n",
    "        self.u = u\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        self.mu = mu\n",
    "        xi = N/p\n",
    "        self.phi = np.arctan((l*np.sqrt(xi))/mu)*(2/np.pi)\n",
    "        self.threshold = l**2 + (mu**2)/(N/p)\n",
    "        self.class1_ids = class1_ids.reshape(-1)\n",
    "        self.class2_ids = class2_ids.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \n",
    "    def __init__(self, local_model, node_idx, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        local model: The local MLP model for each node\n",
    "        node_idx: The unique index of a node\n",
    "        X: [n_k, p], feature matrix, float tensor\n",
    "        y: [n_k], true labels, long tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = local_model\n",
    "        self.idx = node_idx\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_k = X.shape[0]\n",
    "        self.dataloader = None\n",
    "        \n",
    "        \n",
    "    def upload_local_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Upload local model parameters to central server.\n",
    "        Usually used for aggregation step in each communication.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    \n",
    "    def receieve_central_parameters(self, central_parameters):\n",
    "        \n",
    "        \"\"\"\n",
    "        central_parameters: A state dictonary for central server parameters.\n",
    "        \n",
    "        Receive the broadcasted central parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pname, param in self.model.named_parameters():\n",
    "                param.copy_(central_parameters[pname])\n",
    "                \n",
    "                \n",
    "    def upload_h(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        This function uploads an random hidden vector from a node to the central server.\n",
    "        It also calculate and upload a dictonary of gradients  (dh/dw, 3D tensors) for each parameter w.r.t the local model\n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        # x: [p]\n",
    "        x = self.X[np.random.choice(a=self.n_k),:]\n",
    "        \n",
    "        # Clear the possible accumulated gradient of the parameters of local model\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        h = self.model(x).view(1, -1)\n",
    "        \n",
    "        num_class = h.shape[-1]\n",
    "        \n",
    "        dh = {}\n",
    "        \n",
    "        for i in range(num_class):\n",
    "            \n",
    "            h[0, i].backward(retain_graph=True)\n",
    "            \n",
    "            for pname, param in self.model.named_parameters():\n",
    "                \n",
    "                if pname in dh:\n",
    "                    dh[pname].append(param.grad.data.clone())\n",
    "                else:\n",
    "                    dh[pname] = []\n",
    "                    dh[pname].append(param.grad.data.clone())\n",
    "                    \n",
    "                if (i == num_class-1):\n",
    "                    d1, d2 = dh[pname][0].shape\n",
    "                    dh[pname] = torch.cat(dh[pname], dim=0).view(num_class, d1, d2)\n",
    "                    \n",
    "            self.model.zero_grad()\n",
    "        \n",
    "        return h, dh\n",
    "    \n",
    "    def local_update(self, A_tilde_k, C_k, dH, I, pi, \n",
    "                     opt, learning_rate=0.01, num_epochs=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        The local update process for a node k.\n",
    "        A_tilde_k: The kth row of PageRank matrix A_tilde.\n",
    "        C_k: [1, num_class] The aggregated neighborhood information for node k.\n",
    "        dH: A list of gradient dictonaries, where the kth dictonary contains the gradients of each parameter for node k.\n",
    "        I: Number of local updates.\n",
    "        opt: Optimizer used for local updates: SGD or Adam.\n",
    "        learning rate: learning rate for SGD.\n",
    "        \n",
    "        To-Do:\n",
    "        Number of \"epochs\"\n",
    "        Gradient Clipping\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = int(np.floor(self.n_k/I))\n",
    "        \n",
    "        if (self.dataloader == None):\n",
    "            dataset = torch.utils.data.TensorDataset(self.X, self.y)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        k = self.idx\n",
    "        \n",
    "        N = len(dH)\n",
    "        \n",
    "        num_class = C_k.shape[-1]\n",
    "        \n",
    "        if (opt == \"Adam\"):\n",
    "            optimizer = optim.Adam(self.model.parameters())\n",
    "            \n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            node_train_loss = 0\n",
    "\n",
    "            for X_B, y_B in self.dataloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                B = X_B.shape[0]\n",
    "            \n",
    "                H_B = self.model(X_B)\n",
    "                y_B_onehot = torch.zeros(B, num_class)\n",
    "                y_B_onehot[np.arange(B), y_B] = 1\n",
    "\n",
    "                Z_B = A_tilde_k[k]*H_B + C_k\n",
    "                y_B_hat = F.softmax(Z_B, dim=1)\n",
    "                batch_loss = F.nll_loss(torch.log(y_B_hat), y_B, reduction=\"sum\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    Errs = y_B_hat - y_B_onehot\n",
    "                    node_train_loss += batch_loss.item()/B\n",
    "                    \n",
    "                batch_loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for pname, param in self.model.named_parameters():\n",
    "                        for i in range(N):\n",
    "                            if (i != k):\n",
    "                                param.grad.data += A_tilde_k[i]*torch.tensordot(Errs, dH[i][pname], dims=1).sum(dim=0)\n",
    "                        param.grad.data = param.grad.data/B\n",
    "                optimizer.step()\n",
    "                \n",
    "        return node_train_loss/len(self.dataloader)\n",
    "    \n",
    "    def local_eval(self, central_model, A_tilde_kk, C_k):\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            H = central_model(self.X)\n",
    "            y_hat = F.softmax(A_tilde_kk*H+C_k, dim=1)\n",
    "            loss = F.nll_loss(torch.log(y_hat), self.y, reduction=\"mean\")\n",
    "            preds = torch.max(y_hat, dim=1)[1]\n",
    "            count = (preds == self.y).sum().item()\n",
    "            \n",
    "        return loss.item(), count/self.n_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Server Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Central_Server:\n",
    "    \n",
    "    def __init__(self, node_list, A_tilde):\n",
    "        \n",
    "        \"\"\"\n",
    "        A_tilde: PageRank matrix\n",
    "        node_list: A list contains objects from Node class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A_tilde = A_tilde\n",
    "        self.node_list = node_list\n",
    "        self.N = len(node_list)\n",
    "        self.central_parameters = None\n",
    "        self.cmodel = None\n",
    "        \n",
    "    def init_central_parameters(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the central server parameter dictonary\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cmodel = MLP(input_dim, hidden_dim, output_dim)\n",
    "        \n",
    "        self.central_parameters = copy.deepcopy(self.cmodel.state_dict())\n",
    "        \n",
    "        \n",
    "    def broadcast_central_parameters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Broadcast the current central parameters to all nodes.\n",
    "        Usually used after the aggregation in the end of each communication\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.central_parameters == None:\n",
    "            raise ValueError(\"Central parameters is None, Please initilalize it first.\")\n",
    "        \n",
    "        for node in self.node_list:\n",
    "            node.receieve_central_parameters(self.central_parameters)\n",
    "        \n",
    "    def collect_hs(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Collect h and dh from each node\n",
    "        \"\"\"\n",
    "        \n",
    "        H = []\n",
    "        dH = []\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            h_i, dh_i = self.node_list[i].upload_h()\n",
    "            H.append(h_i)\n",
    "            dH.append(dh_i)\n",
    "        \n",
    "        # H: [N, num_class]\n",
    "        H = torch.cat(H, dim=0)\n",
    "        \n",
    "        # dH: a list of gradient dictonaries\n",
    "        return H, dH\n",
    "        \n",
    "        \n",
    "    def communication(self, train_indices, test_indices, I, aggregation, opt, learning_rate, num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        train_indices: A list of indices for the nodes that will be used during training\n",
    "        I: Number of local updates\n",
    "        learning: Learning rate\n",
    "        opt: The name of the optimizer used for training.\n",
    "        aggregation: The function to aggregate the local parameters from the nodes used in training.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.broadcast_central_parameters()\n",
    "        \n",
    "        # H: [N, num_classes]\n",
    "        H, dH = self.collect_hs()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            H_copy = torch.clone(H).detach()\n",
    "            C = torch.matmul(self.A_tilde, H_copy)\n",
    "            \n",
    "        total_train_loss = 0\n",
    "        \n",
    "        total_test_loss = 0\n",
    "        \n",
    "        test_accuracy = 0\n",
    "        \n",
    "        pi = train_indices[0]\n",
    "            \n",
    "        for k in train_indices:\n",
    "            \n",
    "            C_k = C[k,:] - self.A_tilde[k,k]*H_copy[k,:]\n",
    "            \n",
    "            total_train_loss += self.node_list[k].local_update(self.A_tilde[k,:], C_k, dH, \n",
    "                                           I, pi, opt, learning_rate, num_epochs)\n",
    "            \n",
    "        aggregation(self.central_parameters, self.cmodel, self.node_list, train_indices)\n",
    "    \n",
    "        for k in test_indices:\n",
    "            C_k = C[k,:] - self.A_tilde[k,k]*H_copy[k,:]\n",
    "            anloss, naccuracy = self.node_list[k].local_eval(self.cmodel, self.A_tilde[k,k], C_k)\n",
    "            \n",
    "            total_test_loss += anloss\n",
    "            test_accuracy += naccuracy\n",
    "            \n",
    "        \n",
    "        return total_train_loss/len(train_indices), total_test_loss/len(test_indices), test_accuracy/len(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_agg(central_parameters, central_model, node_list, train_indices):\n",
    "    \n",
    "    num_train = len(train_indices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pname, param in central_model.named_parameters():\n",
    "            \n",
    "            p = node_list[train_indices[0]].model.state_dict()[pname]\n",
    "            \n",
    "            for i in range(1, num_train):\n",
    "                \n",
    "                p = p + node_list[train_indices[i]].model.state_dict()[pname]\n",
    "            \n",
    "            p = p/num_train\n",
    "            \n",
    "            central_parameters[pname] = p\n",
    "            \n",
    "            param.copy_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cSBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CSBM(csbm, A_tilde, hidden_dim, \n",
    "               n_k, I, num_communication, aggregation,\n",
    "               num_train, learning_rate=0.1, opt=\"Adam\", num_epochs=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    csbm: An cSBM object (contextual stochastic block model)\n",
    "    A_tilde: pageRank matrix\n",
    "    n_k: number of feature vectors each node has\n",
    "    I: number of local updates for each node\n",
    "    num_communication: Number of communicatons\n",
    "    aggregation: aggregation method, a function\n",
    "    num_train: Number of nodes used to train\n",
    "    learning_rate: Learning rate for SGD\n",
    "    opt: optimization method: Adam or SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    N = A_tilde.shape[0]\n",
    "    \n",
    "    input_dim = csbm.p\n",
    "    \n",
    "    output_dim = 2\n",
    "    \n",
    "    node_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        model_i = MLP(input_dim, hidden_dim, output_dim)\n",
    "        \n",
    "        for j in range(n_k):\n",
    "            \n",
    "            x_j = np.sqrt(csbm.mu/N)*csbm.v[i]*csbm.u + np.random.normal(loc=0, scale=1, size=csbm.p)/np.sqrt(csbm.p)\n",
    "            \n",
    "            X.append(x_j)\n",
    "            \n",
    "        X = torch.tensor(np.array(X))\n",
    "        \n",
    "        if csbm.v[i] == -1:\n",
    "            \n",
    "            y = np.zeros(n_k)\n",
    "            \n",
    "        elif csbm.v[i] == 1:\n",
    "            \n",
    "            y = np.ones(n_k)\n",
    "\n",
    "        y = torch.tensor(y).type(torch.LongTensor)\n",
    "        \n",
    "        node_i = Node(local_model=model_i, node_idx=i, X=X, y=y)\n",
    "        \n",
    "        node_list.append(node_i)\n",
    "        \n",
    "    server = Central_Server(node_list, A_tilde)\n",
    "    \n",
    "    server.init_central_parameters(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    class1_train = np.random.choice(a=csbm.class1_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    class2_train = np.random.choice(a=csbm.class2_ids, size=int(num_train/2), replace=False)\n",
    "    \n",
    "    train_indices = np.concatenate((class1_train, class2_train), axis=0)\n",
    "    \n",
    "    test_indices = list(set(np.arange(N)) - set(train_indices))\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    test_loss = []\n",
    "    \n",
    "    test_acc = []\n",
    "    \n",
    "    for ith in range(num_communication):\n",
    "        \n",
    "        average_train_loss, average_test_loss, test_accuracy  = server.communication(train_indices, test_indices, \n",
    "                                                                        I, aggregation, opt, learning_rate, num_epochs)\n",
    "        train_loss.append(average_train_loss)\n",
    "        \n",
    "        test_loss.append(average_test_loss)\n",
    "        \n",
    "        test_acc.append(test_accuracy)\n",
    "        \n",
    "        print (\"Communication:\", ith+1, \"Average train loss:\", average_train_loss,\n",
    "               \"Average test loss:\", average_test_loss, \"Test accuracy:\", test_accuracy)\n",
    "        \n",
    "    return train_loss, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1 Average train loss: 0.3562433129549026 Average test loss: 0.6971662253141403 Test accuracy: 0.475\n",
      "Communication: 2 Average train loss: 0.3295503455400467 Average test loss: 0.6917898438870906 Test accuracy: 0.4765625\n",
      "Communication: 3 Average train loss: 0.30272941052913666 Average test loss: 0.6819510966539383 Test accuracy: 0.7409374999999999\n",
      "Communication: 4 Average train loss: 0.2715079945325851 Average test loss: 0.6644757330417633 Test accuracy: 0.7315625000000001\n",
      "Communication: 5 Average train loss: 0.25720848739147184 Average test loss: 0.6717058844864369 Test accuracy: 0.7665624999999999\n",
      "Communication: 6 Average train loss: 0.24167072385549546 Average test loss: 0.6772050760686398 Test accuracy: 0.6959374999999999\n",
      "Communication: 7 Average train loss: 0.2209977704286575 Average test loss: 0.6450009368360042 Test accuracy: 0.6190625000000001\n",
      "Communication: 8 Average train loss: 0.19908937662839893 Average test loss: 0.6352919042110443 Test accuracy: 0.8721875000000001\n",
      "Communication: 9 Average train loss: 0.19999684453010558 Average test loss: 0.6627708286046982 Test accuracy: 0.728125\n",
      "Communication: 10 Average train loss: 0.18408019781112667 Average test loss: 0.6435794085264206 Test accuracy: 0.6134375\n",
      "Communication: 11 Average train loss: 0.1710695758461952 Average test loss: 0.6317848108708859 Test accuracy: 0.8178124999999998\n",
      "Communication: 12 Average train loss: 0.15322884231805806 Average test loss: 0.5882983520627022 Test accuracy: 0.8271875\n",
      "Communication: 13 Average train loss: 0.15242468267679216 Average test loss: 0.6175231836736202 Test accuracy: 0.7925000000000002\n",
      "Communication: 14 Average train loss: 0.1419571231305599 Average test loss: 0.606710771098733 Test accuracy: 0.8512499999999997\n",
      "Communication: 15 Average train loss: 0.13271855816245076 Average test loss: 0.5928491935133934 Test accuracy: 0.8237499999999999\n",
      "Communication: 16 Average train loss: 0.13476235017180443 Average test loss: 0.6173216138035059 Test accuracy: 0.6450000000000001\n",
      "Communication: 17 Average train loss: 0.1198965160548687 Average test loss: 0.5569607280194759 Test accuracy: 0.726875\n",
      "Communication: 18 Average train loss: 0.1016377115249634 Average test loss: 0.5195136241614818 Test accuracy: 0.9484375000000002\n",
      "Communication: 19 Average train loss: 0.11904171362519264 Average test loss: 0.6099937841296196 Test accuracy: 0.6181249999999999\n",
      "Communication: 20 Average train loss: 0.09430319488048552 Average test loss: 0.5073341686278582 Test accuracy: 0.9425000000000001\n"
     ]
    }
   ],
   "source": [
    "a,b,c = train_CSBM(csbm=csbm, A_tilde=A_tilde, hidden_dim=200, n_k=40,\n",
    "           I=2, num_communication=20, aggregation=mean_agg, num_train=20, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001680340923813"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "p = 10\n",
    "d = 5\n",
    "mu = 1\n",
    "l = 2\n",
    "csbm = cSBM(N, p, d, mu, l)\n",
    "A_tilde = calculate_Atilde(csbm.A, 100, 0.95)\n",
    "csbm.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x161985d2370>]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
